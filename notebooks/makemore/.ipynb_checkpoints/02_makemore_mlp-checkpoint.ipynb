{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b7368-58b1-47e3-88f9-f3d5607d41d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b49783-4e30-4bfb-a563-838b8e62deae",
   "metadata": {},
   "source": [
    "# Makemore "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43bccc2-7fee-4b22-ba4c-0188ac01ea0a",
   "metadata": {},
   "source": [
    "## Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc754a07-136c-40f0-8da2-a91c53ace2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa273a80-c43e-494d-8189-400e5c1b6607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "vocab = sorted(set('.' + ''.join(words).lower()))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9818bdb6-e37d-4dd4-87cc-6836bc2a5200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, y = [], []\n",
    "  for word in words:\n",
    "    context = [0] * block_size\n",
    "    for char in word + '.':\n",
    "      idx = char_to_idx[char]\n",
    "      X.append(context)\n",
    "      y.append(idx)\n",
    "      context = context[1:] + [idx]\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  y = torch.tensor(y)\n",
    "  print(X.shape, y.shape)\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d00f2-3a03-40e4-898f-09096430941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = build_dataset(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540baa92-eb24-4d06-a635-d877f61b8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87baf86-3557-4294-ac85-9e8929f502c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "VAL_SIZE = 0.1\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "def train_val_test_split(X, y, train_size):\n",
    "    \"\"\"Split dataset into data splits.\"\"\"\n",
    "    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26b726-f24b-4959-a6ab-832ddd124191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X=X, y=y, train_size=TRAIN_SIZE)\n",
    "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262c834-4576-4b42-809d-73e920edd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27, 10), generator=g)\n",
    "W1 = torch.randn((30, 200), generator=g)\n",
    "b1 = torch.randn(200, generator=g)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631caef3-2bb6-4187-8e3d-dfc00bf02afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.nelement() for p in parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db4a7a-f0d1-47ce-bc08-f6759a0cf620",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ecce1-480c-44bd-b403-73d35bda08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre\n",
    "lri = []\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "for i in range(10000):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, X_train.shape[0], (32,))\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[X_train[ix]] # (32, 3, 10)\n",
    "  h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 200)\n",
    "  logits = h @ W2 + b2 # (32, 27)\n",
    "  loss = F.cross_entropy(logits, y_train[ix])\n",
    "  #print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  #lr = lrs[i]\n",
    "  lr = 0.1 if i < 100000 else 0.01\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  #lri.append(lre[i])\n",
    "  stepi.append(i)\n",
    "  lossi.append(loss.log10().item())\n",
    "\n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dad40e-f89a-4110-b3ea-acbdadf69cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepi, lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd170848-c1da-481b-8430-6654f113c3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X_train] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc4da2-d626-4ee5-ae65-556a358a82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(emb.view(-1, 30) @ W1 + b1) # (32, 100)\n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "loss = F.cross_entropy(logits, )\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2262a6b4-571a-4e95-bc25-672d94321f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), idx_to_char[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9ffc7-659b-49ea-8b49-79d55da56d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "      logits = h @ W2 + b2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(idx_to_char[i] for i in out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968d33c-6e21-4e6f-a2c4-4ab4f7892429",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62abc1a-e485-413e-99dd-e1aa1da6c8d1",
   "metadata": {},
   "source": [
    "### Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bdf87-efce-4bca-a2a1-39819a8832e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('names.txt', 'r').read().splitlines()\n",
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8c7a2-854e-4ffd-9f72-bbb31331ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d4596-9127-43f8-a3ba-f147e62fb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set('.' + ''.join(names).lower()))\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2913809-1d49-4720-9f7f-c0a8c258d625",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041f3dd9-a0ee-4fd9-a52f-7221192b3280",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db69fee-cb8d-45d7-a466-7b6b2e015c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "block_size = 3\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, y = [], []\n",
    "    for word in words:\n",
    "        context = [0] * block_size\n",
    "        for char in word + '.':\n",
    "            X.append(context)\n",
    "            y.append(char_to_idx[char])\n",
    "            #print(''.join(idx_to_char[i] for i in context), '--->', idx_to_char[char_to_idx[char]])\n",
    "            context = context[1:] + [char_to_idx[char]]\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    print(X.shape, y.shape)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcc127-419a-4aa4-ba3c-ed735f50e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = build_dataset(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c9ee0-8e3f-4c58-9e35-fc04b4e269d5",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7b6e15-26ba-4f84-9454-123180fab879",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "VAL_SIZE = 0.1\n",
    "TEST_SIZE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fa2ac-3d9e-4a0c-8697-cee7352839d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = len(X)\n",
    "indices = list(range(NUM_SAMPLES))\n",
    "np.random.shuffle(indices)\n",
    "X = X[indices]\n",
    "y = y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeaa885-8654-4fec-a281-343be83343f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split indices\n",
    "train_start = 0\n",
    "train_end = int(0.7*NUM_SAMPLES)\n",
    "val_start = train_end\n",
    "val_end = int((TRAIN_SIZE+VAL_SIZE)*NUM_SAMPLES)\n",
    "test_start = val_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed2173-2716-4f62-8629-82f7baee3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train = X[train_start:train_end]\n",
    "y_train = y[train_start:train_end]\n",
    "X_val = X[val_start:val_end]\n",
    "y_val = y[val_start:val_end]\n",
    "X_test = X[test_start:]\n",
    "y_test = y[test_start:]\n",
    "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print (f\"X_val: {X_val.shape}, y_test: {y_val.shape}\")\n",
    "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898c5ad7-c4c0-482d-a4af-9dd6ace2ec4f",
   "metadata": {},
   "source": [
    "### Init Weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b46db-fe63-4353-bb2d-ac26419a87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_parameters(vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "    return {\n",
    "        'C': np.random.randn(vocab_size, embedding_dim) * 0.01,\n",
    "        'W1': np.random.randn(3 * embedding_dim, hidden_dim) * 0.01,\n",
    "        'b1': np.zeros((1, hidden_dim)),\n",
    "        'W2': np.random.randn(hidden_dim, output_dim) * 0.01,\n",
    "        'b2': np.zeros((1, output_dim))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c5611-e022-465d-afb5-1eaa952bdb85",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2542df60-5606-4d35-bb83-deace71b616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(logits, y):\n",
    "    probs = softmax(logits)\n",
    "    return -np.mean(np.log(probs[np.arange(len(y)), y]))\n",
    "\n",
    "def forward(X, params):\n",
    "    emb = params['C'][X]  # (batch_size, 3, embedding_dim)\n",
    "    h = tanh(np.reshape(emb, (emb.shape[0], -1)) @ params['W1'] + params['b1'])\n",
    "    logits = h @ params['W2'] + params['b2']\n",
    "    return emb, h, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f1a7b-8119-42cc-9b3a-cdc32a631c4e",
   "metadata": {},
   "source": [
    "### Loss, Gradients and Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b091c-4e5a-47e5-8916-1b2064aeb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, y, emb, h, logits, params):\n",
    "    batch_size = X.shape[0]\n",
    "    \n",
    "    probs = softmax(logits)\n",
    "    dlogits = probs\n",
    "    dlogits[np.arange(batch_size), y] -= 1\n",
    "    dlogits /= batch_size\n",
    "    \n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = np.sum(dlogits, axis=0, keepdims=True)\n",
    "    dh = dlogits @ params['W2'].T\n",
    "    \n",
    "    demb = (dh * (1 - h**2)) @ params['W1'].T\n",
    "    demb = np.reshape(demb, (batch_size, 3, -1))\n",
    "    \n",
    "    dW1 = np.reshape(emb, (batch_size, -1)).T @ (dh * (1 - h**2))\n",
    "    db1 = np.sum(dh * (1 - h**2), axis=0, keepdims=True)\n",
    "    \n",
    "    dC = np.zeros_like(params['C'])\n",
    "    for i in range(batch_size):\n",
    "        for j in range(3):\n",
    "            dC[X[i, j]] += demb[i, j]\n",
    "    \n",
    "    return {'C': dC, 'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "\n",
    "def update_parameters(params, grads, lr):\n",
    "    for key in params:\n",
    "        params[key] -= lr * grads[key]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4440a8c-4fd5-4c1f-9f9f-11f090ec9564",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccab363-30f5-463e-9068-b17e1759b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X, y, params, lr):\n",
    "    emb, h, logits = forward(X, params)\n",
    "    loss = cross_entropy_loss(logits, y)\n",
    "    grads = backward(X, y, emb, h, logits, params)\n",
    "    params = update_parameters(params, grads, lr)\n",
    "    return loss, params\n",
    "\n",
    "def train_model(Xtr, Ytr, vocab_size, embedding_dim, hidden_dim, output_dim, num_iterations, batch_size):\n",
    "    params = init_parameters(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "    losses = []\n",
    "    steps = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        ix = np.random.randint(0, Xtr.shape[0], (batch_size,))\n",
    "        X_batch, y_batch = Xtr[ix], Ytr[ix]\n",
    "        \n",
    "        lr = 0.1 if i < 100000 else 0.01\n",
    "        loss, params = train_step(X_batch, y_batch, params, lr)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            losses.append(np.log10(loss))\n",
    "            steps.append(i)\n",
    "            print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "\n",
    "    return params, losses, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658af62-cad6-431c-91fb-834f2c68bda6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 10 # dimensionality of the character embedding vectors.\n",
    "hidden_dim = 200 # number of neurons in the hidden layer\n",
    "output_dim = 27  # Adjust based on your output size\n",
    "num_iterations = 200000\n",
    "batch_size = 32\n",
    "\n",
    "trained_params, losses, steps = train_model(X_train, y_train, \n",
    "                                            vocab_size, embedding_dim, \n",
    "                                            hidden_dim, output_dim, \n",
    "                                            num_iterations, batch_size)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.plot(steps, losses)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Log10 Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268b6a8-a8b6-4bfb-824d-a960e19ee0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(X, Y, batch_size):\n",
    "    indices = np.random.randint(0, X.shape[0], batch_size)\n",
    "    return X[indices], Y[indices]\n",
    "\n",
    "def plot_histogram(h, bins=50):\n",
    "    flattened_h = np.reshape(h, -1)  # Flatten h\n",
    "    plt.hist(flattened_h, bins=bins)\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Histogram of Flattened h')\n",
    "    plt.show()\n",
    "\n",
    "X_batch, _ = get_minibatch(X_train, y_train, batch_size)\n",
    "_, h, _ = forward(X_batch, params)\n",
    "plot_histogram(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e3d1f-80c2-4acb-838d-2d47390b0776",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9078ffb0-fa01-434f-8b72-4f860372b2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), idx_to_char[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89dd640-9cca-4efc-91e2-5393be52e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name(params, vocab_size, block_size, idx_to_char):\n",
    "    C, W1, b1, W2, b2 = params['C'], params['W1'], params['b1'], params['W2'], params['b2']\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size  # initialize with all ...\n",
    "    \n",
    "    while True:\n",
    "        emb = C[context]  # (block_size, d)\n",
    "        h = np.tanh(emb.reshape(1, -1) @ W1 + b1)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = softmax(logits)\n",
    "        ix = np.random.choice(vocab_size, p=probs.ravel())\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    return ''.join(idx_to_char[i] for i in out)\n",
    "\n",
    "# Generate 20 names\n",
    "for _ in range(20):\n",
    "    generated_name = generate_name(trained_params, vocab_size, block_size, idx_to_char)\n",
    "    print(generated_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
