{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27c511-852c-46aa-b59b-21fb7e780806",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfb7a6-c26c-4c91-88db-8f1396265be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import itertools\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mwml.utils import LabelEncoder, Tokenizer, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f938d-6d58-4be2-95a8-43e66e1c3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "def set_seeds(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b712961-dde5-48db-8609-4ffd040f4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "set_seeds(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98415993-c2e4-4421-82be-0cf7018cde00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "cuda = True\n",
    "device = torch.device(\"cuda\" if (\n",
    "    torch.cuda.is_available() and cuda) else \"cpu\")\n",
    "torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a0fa60-4399-429a-b49f-d1cd0aa511f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Convolutional Neural Netwoks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66252595-c2f2-4df1-bfe6-e7eef35eda32",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d3c43-3585-4c2f-a4b6-3416244ab467",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/fancyzhx/ag_news/\" + splits[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8552833-07f4-41f5-bdec-95086f29a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: 'World',\n",
    "    1: 'Sports',\n",
    "    2: 'Business',\n",
    "    3: 'Sci/Tech'\n",
    "}\n",
    "\n",
    "df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601791d2-80b0-49c7-ac9d-2f1f0d2b770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450a4a8-bfaa-4d66-8991-42b95a2314bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True) # shuffle\n",
    "df.shape, df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91500a2f-d462-4e08-b027-4e165443bc0b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cc4dac-0a5a-40aa-a124-4d73d5733f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = stopwords.words(\"english\")\n",
    "print(STOPWORDS[:5])\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6dbca2-d658-4754-8a2b-f08704df2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text, stopwords=STOPWORDS):\n",
    "    \"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove stopwords\n",
    "    pattern = re.compile(r\"\\b(\" + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
    "    text = pattern.sub(\"\", text)\n",
    "\n",
    "    # Remove words in parenthesis\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n",
    "\n",
    "    # Spacing and filters\n",
    "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)  # separate punctuation tied to words\n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  # remove non alphanumeric chars\n",
    "    text = re.sub(\" +\", \" \", text)  # remove multiple spaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51835c09-92e5-4d4e-b7a6-a824c7a3f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Great week for the NYSE!\"\n",
    "preprocess(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3f463-5f81-4748-8d99-c8e39734e835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to dataframe\n",
    "preprocessed_df = df.copy()\n",
    "preprocessed_df.text = preprocessed_df.text.apply(preprocess)\n",
    "print(f\"{df.text.values[0]}\\n\\n{preprocessed_df.text.values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a31792-861a-4b5c-ac3a-832108312356",
   "metadata": {},
   "source": [
    "## Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bac260-2018-46fe-b24e-b28cdd27a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.7\n",
    "VAL_SIZE = 0.15\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef11f71-d712-47de-b0e9-18766cdf38b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(X, y, train_size):\n",
    "    \"\"\"Split dataset into data splits.\"\"\"\n",
    "    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765e972e-1847-4fdd-8375-df3ccb836eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = preprocessed_df[\"text\"].values\n",
    "y = preprocessed_df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bd59f-2cc8-4141-8055-4739898a308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data splits\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
    "    X=X, y=y, train_size=TRAIN_SIZE)\n",
    "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "print (f\"Sample point: {X_train[0]} → {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad807e-3f0c-4f33-bf17-29ee4197e8c4",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364d0c00-6010-4259-bbd5-c1fbaff223e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "NUM_CLASSES = len(label_encoder)\n",
    "label_encoder.class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dccaa0-4248-436f-9177-4924cb56c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to tokens\n",
    "print (f\"y_train[0]: {y_train[0]}\")\n",
    "y_train = label_encoder.encode(y_train)\n",
    "y_val = label_encoder.encode(y_val)\n",
    "y_test = label_encoder.encode(y_test)\n",
    "print (f\"y_train[0]: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab570e-064b-4f22-a3cd-889a72f292a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights\n",
    "counts = np.bincount(y_train)\n",
    "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
    "print (f\"counts: {counts}\\nweights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832af507-d37f-4e66-9e83-cc095bc1f24e",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8177c8-e3ee-42f2-a92e-8bec30939189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenizer = Tokenizer(char_level=False, num_tokens=500)\n",
    "tokenizer.fit_on_texts(texts=X_train)\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c42d1b-fd13-4fe2-89c0-cb8e34f69498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def take(n, iterable):\n",
    "    \"\"\"Return first *n* items of the iterable as a list.\n",
    "\n",
    "        >>> take(3, range(10))\n",
    "        [0, 1, 2]\n",
    "\n",
    "    If there are fewer than *n* items in the iterable, all of them are\n",
    "    returned.\n",
    "\n",
    "        >>> take(10, range(3))\n",
    "        [0, 1, 2]\n",
    "\n",
    "    \"\"\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1cdb5-8257-404d-a6a6-3746794fcd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(take(5, tokenizer.token_to_index.items()))\n",
    "print(f\"least freq token's freq: {tokenizer.min_token_freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd06bb8-a112-4417-b8c9-b250b1b16c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert texts to sequences of indices\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
    "print (\"Text to indices:\\n\"\n",
    "    f\"  (preprocessed) → {preprocessed_text}\\n\"\n",
    "    f\"  (tokenized) → {X_train[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42289d-8202-45fb-a8e2-d95c123db6d2",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44df4cb-466b-4646-8aae-aa5e445cb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(seq, num_classes):\n",
    "    \"\"\"One-hot encode a sequence of tokens.\"\"\"\n",
    "    one_hot = np.zeros((len(seq), num_classes))\n",
    "    for i, item in enumerate(seq):\n",
    "        one_hot[i, item] = 1.\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714e0ad-79fe-466d-b827-575314f1758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])\n",
    "print(len(X_train[0]))\n",
    "cat = to_categorical(seq=X_train[0], num_classes=len(tokenizer))\n",
    "print(cat)\n",
    "print(cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a6db7-c39f-44c1-80b0-67ad24d6d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "X_train = [to_categorical(seq, num_classes=vocab_size) for seq in X_train]\n",
    "X_val = [to_categorical(seq, num_classes=vocab_size) for seq in X_val]\n",
    "X_test = [to_categorical(seq, num_classes=vocab_size) for seq in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ccf52f-e3db-49d4-a14d-7339ff7d9af0",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab08f305-5d91-4539-8875-dc3e0e391e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_seq_len=0):\n",
    "    \"\"\"Pad sequences to max length in sequence.\"\"\"\n",
    "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
    "    num_classes = sequences[0].shape[-1]\n",
    "    padded_sequences = np.zeros((len(sequences), max_seq_len, num_classes))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        padded_sequences[i][:len(sequence)] = sequence\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b364c-3372-4303-bc7e-6a2f5c63efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D sequences\n",
    "print (X_train[0].shape, X_train[1].shape, X_train[2].shape)\n",
    "padded = pad_sequences(X_train[0:3])\n",
    "print (padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fc910b-c88e-40c2-89f8-ad51ba9f4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SIZE = 1\n",
    "# Create datasets for embedding\n",
    "train_dataset = Dataset(X=X_train, y=y_train, max_filter_size=FILTER_SIZE)\n",
    "val_dataset = Dataset(X=X_val, y=y_val, max_filter_size=FILTER_SIZE)\n",
    "test_dataset = Dataset(X=X_test, y=y_test, max_filter_size=FILTER_SIZE)\n",
    "print (\"Datasets:\\n\"\n",
    "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
    "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
    "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {test_dataset[0][0]}\\n\"\n",
    "    f\"  y: {test_dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6e8b3f-5c6d-4b52-abd2-4653c774c837",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf92d1-5d4c-4b01-8e0b-b27ea48e1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "train_dataloader = train_dataset.create_dataloader(batch_size=batch_size)\n",
    "val_dataloader = val_dataset.create_dataloader(batch_size=batch_size)\n",
    "test_dataloader = test_dataset.create_dataloader(batch_size=batch_size)\n",
    "batch_X, batch_y = next(iter(test_dataloader))\n",
    "print (\"Sample batch:\\n\"\n",
    "    f\"  X: {list(batch_X.size())}\\n\"\n",
    "    f\"  y: {list(batch_y.size())}\\n\"\n",
    "    \"Sample point:\\n\"\n",
    "    f\"  X: {batch_X[0]}\\n\"\n",
    "    f\"  y: {batch_y[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7965d-016d-4831-a51d-26ef048e5649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
